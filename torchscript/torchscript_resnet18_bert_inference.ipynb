{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "With TorchScript, PyTorch aims to create a unified framework from research to production. TorchScript takes our PyTorch modules as input and convert them into a production-friendly format."
      ],
      "metadata": {
        "id": "RqzQhnBzVeE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To focus on the production use case, PyTorch uses 'Script mode' which has 2 components PyTorch JIT and TorchScript."
      ],
      "metadata": {
        "id": "E47sBQANVjoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YoG-ecaUOqr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import torch\n",
        "from time import perf_counter"
      ],
      "metadata": {
        "id": "ovHKZAJhMBSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timer(f,*args):\n",
        "\n",
        "    start = perf_counter()\n",
        "    f(*args)\n",
        "    return (1000 * (perf_counter() - start))\n",
        "\n",
        "script_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', torchscript=True)\n",
        "script_model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
        "\n",
        "\n",
        "# Tokenizing input text\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = script_tokenizer.tokenize(text)\n",
        "\n",
        "# Masking one of the input tokens\n",
        "masked_index = 8\n",
        "\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "indexed_tokens = script_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Creating a dummy input\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "metadata": {
        "id": "4lNMxXUuLAgT",
        "outputId": "9338fba6-1732-4919-ffe6-53b752140b73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT on CPU\n",
        "native_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "np.mean([timer(native_model,tokens_tensor,segments_tensors) for _ in range(100)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Z1Ov8GPrxc",
        "outputId": "cdf13766-728c-40e3-d825-eb633ae05186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.82381234999514"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT on GPU\n",
        "# Both sample data model need be on the GPU device for the inference to take place\n",
        "native_gpu = native_model.cuda()\n",
        "tokens_tensor_gpu = tokens_tensor.cuda()\n",
        "segments_tensors_gpu = segments_tensors.cuda()\n",
        "np.mean([timer(native_gpu,tokens_tensor_gpu,segments_tensors_gpu) for _ in range(100)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh_wlaU-THUE",
        "outputId": "ea23605b-2274-4d55-b762-0cf397d805a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18.771747219999497"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script mode is invoked by either torch.jit.trace or torch.jit.script. Here, I am using trace method."
      ],
      "metadata": {
        "id": "m5-ZvoN7XN16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.jit.trace on CPU\n",
        "traced_model = torch.jit.trace(script_model, [tokens_tensor, segments_tensors])\n",
        "np.mean([timer(traced_model,tokens_tensor,segments_tensors) for _ in range(100)])"
      ],
      "metadata": {
        "id": "Xc4TJkv8WOcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802eb668-55cf-4473-cc77-50e6bf311a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.93036488000462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.jit.trace on GPU\n",
        "traced_model_gpu = torch.jit.trace(script_model.cuda(), [tokens_tensor.cuda(), segments_tensors.cuda()])\n",
        "np.mean([timer(traced_model_gpu,tokens_tensor.cuda(),segments_tensors.cuda()) for _ in range(100)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKLAEavmQFcP",
        "outputId": "9921d8fd-dbac-4e98-d0a0-3d0ba540e778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.323610329995063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traced_model.code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "wWXoX9ifWyN2",
        "outputId": "e3a889e4-1006-43c1-ea7e-220c05e84947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def forward(self,\\n    input_ids: Tensor,\\n    attention_mask: Tensor) -> Tuple[Tensor, Tensor]:\\n  pooler = self.pooler\\n  encoder = self.encoder\\n  embeddings = self.embeddings\\n  embeddings0 = self.embeddings\\n  token_type_ids = embeddings0.token_type_ids\\n  batch_size = ops.prim.NumToTensor(torch.size(input_ids, 0))\\n  _0 = int(batch_size)\\n  seq_length = ops.prim.NumToTensor(torch.size(input_ids, 1))\\n  _1 = int(seq_length)\\n  _2 = int(seq_length)\\n  _3 = torch.slice(token_type_ids, 0, 0, 9223372036854775807)\\n  buffered_token_type_ids = torch.slice(_3, 1, 0, _2)\\n  input = torch.expand(buffered_token_type_ids, [_0, _1])\\n  _4 = torch.slice(attention_mask, 0, 0, 9223372036854775807)\\n  _5 = torch.unsqueeze(torch.unsqueeze(_4, 1), 2)\\n  extended_attention_mask = torch.slice(_5, 3, 0, 9223372036854775807)\\n  _6 = torch.rsub(torch.to(extended_attention_mask, 6), 1.)\\n  attention_mask0 = torch.mul(_6, CONSTANTS.c0)\\n  _7 = (embeddings).forward(input_ids, input, )\\n  _8 = (encoder).forward(_7, attention_mask0, )\\n  return (_8, (pooler).forward(_8, ))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traced_model_gpu.code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "9yZv6u1-Tbfq",
        "outputId": "21b2405d-50f5-41fc-b8e9-fafc09c1f39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def forward(self,\\n    input_ids: Tensor,\\n    attention_mask: Tensor) -> Tuple[Tensor, Tensor]:\\n  pooler = self.pooler\\n  encoder = self.encoder\\n  embeddings = self.embeddings\\n  embeddings0 = self.embeddings\\n  token_type_ids = embeddings0.token_type_ids\\n  batch_size = ops.prim.NumToTensor(torch.size(input_ids, 0))\\n  _0 = int(batch_size)\\n  seq_length = ops.prim.NumToTensor(torch.size(input_ids, 1))\\n  _1 = int(seq_length)\\n  _2 = int(seq_length)\\n  _3 = torch.slice(token_type_ids, 0, 0, 9223372036854775807)\\n  buffered_token_type_ids = torch.slice(_3, 1, 0, _2)\\n  input = torch.expand(buffered_token_type_ids, [_0, _1])\\n  _4 = torch.slice(attention_mask, 0, 0, 9223372036854775807)\\n  _5 = torch.unsqueeze(torch.unsqueeze(_4, 1), 2)\\n  extended_attention_mask = torch.slice(_5, 3, 0, 9223372036854775807)\\n  _6 = torch.rsub(torch.to(extended_attention_mask, 6), 1.)\\n  attention_mask0 = torch.mul(_6, CONSTANTS.c0)\\n  _7 = (embeddings).forward(input_ids, input, )\\n  _8 = (encoder).forward(_7, attention_mask0, )\\n  return (_8, (pooler).forward(_8, ))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from time import perf_counter\n",
        "import numpy as np\n",
        "\n",
        "def timer(f,*args):\n",
        "    start = perf_counter()\n",
        "    f(*args)\n",
        "    return (1000 * (perf_counter() - start))"
      ],
      "metadata": {
        "id": "VSZ-93I3NHox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet on CPU\n",
        "model_ft = torchvision.models.resnet18()\n",
        "model_ft.eval()\n",
        "x_ft = torch.rand(1,3, 224,224)\n",
        "np.mean([timer(model_ft,x_ft) for _ in range(10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU6vjoLhSigA",
        "outputId": "30b742ce-35fc-4f9e-bf45-6812d30fe1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92.92151069999545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet on GPU\n",
        "model_ft_gpu = torchvision.models.resnet18(pretrained=True).cuda()\n",
        "x_ft_gpu = x_ft.cuda()\n",
        "model_ft_gpu.eval()\n",
        "np.mean([timer(model_ft_gpu,x_ft_gpu) for _ in range(10)])"
      ],
      "metadata": {
        "id": "-Ld8K4ySNVLg",
        "outputId": "113471e9-9962-4969-c1c0-6298f20b0a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.044108600010077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script mode is invoked by either torch.jit.trace or torch.jit.script. Here, I am using script method."
      ],
      "metadata": {
        "id": "DowaC4M0Xefj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.jit.script on CPU\n",
        "script_cell = torch.jit.script(model_ft, (x_ft))\n",
        "np.mean([timer(script_cell,x_ft) for _ in range(10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEByAtN0TQg6",
        "outputId": "27dbb0ad-dc21-4ced-a7a1-f5d919c255e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/jit/_script.py:1244: UserWarning: `optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead\n",
            "  \"`optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89.58781770000996"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.jit.script on GPU\n",
        "script_cell_gpu = torch.jit.script(model_ft_gpu, (x_ft_gpu))\n",
        "np.mean([timer(script_cell_gpu,x_ft.cuda()) for _ in range(100)])"
      ],
      "metadata": {
        "id": "YiVMdzE2NZYm",
        "outputId": "c19d23be-6491-46d8-d3a6-a0e281ea33b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.527740690003384"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script_cell.code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T6dAoj1FW5SJ",
        "outputId": "b7426b4c-740e-4511-b349-caef6f5d6d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def forward(self,\\n    x: Tensor) -> Tensor:\\n  return (self)._forward_impl(x, )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script_cell_gpu.code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UtBh93-tTg3n",
        "outputId": "098f6abd-18b8-4f52-a860-702571279439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def forward(self,\\n    x: Tensor) -> Tensor:\\n  return (self)._forward_impl(x, )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(traced_model,'traced_bert.pt')"
      ],
      "metadata": {
        "id": "TDuQwinQNh-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = torch.jit.load('traced_bert.pt')"
      ],
      "metadata": {
        "id": "UWRU3KogNiFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}