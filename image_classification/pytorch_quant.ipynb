{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c54975c6",
      "metadata": {
        "id": "c54975c6"
      },
      "source": [
        "# Inference Optimization for Convolutional Netwroks\n",
        "### Part 1: Model fusion, quantization\n",
        "\n",
        "reference: https://towardsdatascience.com/inference-optimization-for-convolutional-neural-networks-e63b51b0b519\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21dd26c",
      "metadata": {
        "id": "a21dd26c"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76b5ed4",
      "metadata": {
        "id": "a76b5ed4"
      },
      "source": [
        "### Notebook overview\n",
        "- Create CNN model and the quantized version of the same model\n",
        "- Compare difference in size and latency of two models\n",
        "- Fuse several blocks into one\n",
        "- Compare fused and quantized version with only fused version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a235c9f7",
      "metadata": {
        "id": "a235c9f7"
      },
      "source": [
        "### Create simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb803a99",
      "metadata": {
        "id": "fb803a99"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20,kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Convolutional  Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(in_features=50*53*53, out_features=500)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fully connected 2\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.Softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass the input through block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # pass the input through block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # flatten the output from the previous layer and pass it through fully connected 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # pass the input through fully connected 2 and Softmax\n",
        "        x = self.fc2(x)\n",
        "        output = self.Softmax(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create quantized version of CNN"
      ],
      "metadata": {
        "id": "LgFxl58umCq7"
      },
      "id": "LgFxl58umCq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dd9750",
      "metadata": {
        "id": "32dd9750"
      },
      "outputs": [],
      "source": [
        "# changes in network\n",
        "\n",
        "class NetQuant(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetQuant, self).__init__()\n",
        "        # Prepare for quanitzation\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20,kernel_size=(5, 5))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(in_features=50*53*53, out_features=500)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fully connected 2\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=10)\n",
        "        self.Softmax = nn.Softmax(1)\n",
        "\n",
        "        # Prepare for dequantization\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # pass the input through block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # pass the input through block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # flatten the output from the previous layer and pass it through fully connected 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # pass the input through fully connected 2 and Softmax\n",
        "        x = self.fc2(x)\n",
        "        x = self.dequant(x)\n",
        "        x = self.Softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c1f4d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c1f4d1",
        "outputId": "7f425350-06ea-4bab-9778-e1df94fb446a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetQuant(\n",
              "  (quant): QuantStub()\n",
              "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
              "  (Softmax): Softmax(dim=1)\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Define original and quantized models and prepae for evaluation\n",
        "\n",
        "net = Net()\n",
        "net.eval()\n",
        "net_quant = NetQuant()\n",
        "net_quant.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ca95c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ca95c5",
        "outputId": "45aba682-c771-43b6-dfb1-0d737db74e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Prepare model quantization and convert to quantized version\n",
        "net_quant.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "torch.backends.quantized.engine = \"fbgemm\"\n",
        "net_quant = torch.quantization.prepare(net_quant.cpu(), inplace=False)\n",
        "net_quant = torch.quantization.convert(net_quant, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07c2e55",
      "metadata": {
        "id": "b07c2e55"
      },
      "source": [
        "### Check size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c423f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44c423f6",
        "outputId": "c5808a23-614d-4df5-a205-2b110cc41519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size without quantization: 281 MB \n",
            " Size with quantization: 70 MB\n",
            "Size ratio: 4.01\n"
          ]
        }
      ],
      "source": [
        "# Check model size\n",
        "def print_model_size(mdl):\n",
        "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
        "    size = round(os.path.getsize(\"tmp.pt\")/1e6)\n",
        "    os.remove('tmp.pt')\n",
        "    return size\n",
        "\n",
        "net_size = print_model_size(net)\n",
        "quant_size = print_model_size(net_quant)\n",
        "\n",
        "print(f'Size without quantization: {net_size} MB \\n Size with quantization: {quant_size} MB')\n",
        "print(f'Size ratio: {round(net_size/quant_size, 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the size of the model without quantization is 4 times the size with quantization."
      ],
      "metadata": {
        "id": "rdsaloF_mR4f"
      },
      "id": "rdsaloF_mR4f"
    },
    {
      "cell_type": "markdown",
      "id": "a4ce6cb3",
      "metadata": {
        "id": "a4ce6cb3"
      },
      "source": [
        "## Latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c73a652",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c73a652",
        "outputId": "6e49157c-daa2-4a38-ca34-68de86848888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floating point FP32\n",
            "1.42 s ± 193 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Quantized INT8\n",
            "741 ms ± 13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "# input for the model\n",
        "inpp = torch.rand(32, 3, 224, 224)\n",
        "\n",
        "# compare the performance\n",
        "print(\"Floating point FP32\")\n",
        "%timeit net(inpp)\n",
        "\n",
        "print(\"Quantized INT8\")\n",
        "%timeit net_quant(inpp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61176f0",
      "metadata": {
        "id": "c61176f0"
      },
      "source": [
        "### Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdd7325",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdd7325",
        "outputId": "ad843ca3-f7b5-434c-eae7-cf089eed10e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "  (Softmax): Softmax(dim=1)\n",
            ")\n",
            "NetQuant(\n",
            "  (quant): QuantStub()\n",
            "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=140450, out_features=500, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "  (Softmax): Softmax(dim=1)\n",
            "  (dequant): DeQuantStub()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define original and quantized models and prepare for evaluation\n",
        "\n",
        "net = Net()\n",
        "print(net.eval())\n",
        "\n",
        "net_quant = NetQuant()\n",
        "print(net_quant.eval())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac0e44e",
      "metadata": {
        "id": "6ac0e44e"
      },
      "outputs": [],
      "source": [
        "# Perpare blocks for the fusion\n",
        "\n",
        "moduls_to_fuse =  [['conv1', 'relu1'],\n",
        "                   ['conv2', 'relu2'],\n",
        "                   ['fc1', 'relu3']]\n",
        "\n",
        "net_quant_fused = torch.quantization.fuse_modules(net_quant, moduls_to_fuse)\n",
        "\n",
        "net_fused = torch.quantization.fuse_modules(net, moduls_to_fuse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c1672f",
      "metadata": {
        "id": "12c1672f"
      },
      "outputs": [],
      "source": [
        "# Prepare and quantize the model\n",
        "\n",
        "net_quant_fused.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "torch.backends.quantized.engine = \"fbgemm\"\n",
        "net_quant_fused = torch.quantization.prepare(net_quant_fused.cpu(), inplace=False)\n",
        "net_quant_fused = torch.quantization.convert(net_quant_fused, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521f4da1",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "521f4da1",
        "outputId": "a5f48ea1-c8f7-4269-94ac-1e5598793bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused and quantized model latency\n",
            "760 ms ± 79 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "Fused model latency\n",
            "1.45 s ± 220 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "print(\"Fused and quantized model latency\")\n",
        "%timeit net_quant_fused(inpp)\n",
        "\n",
        "print(\"Fused model latency\")\n",
        "%timeit net_fused(inpp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCo9LIA8la0I"
      },
      "id": "tCo9LIA8la0I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New PyTorch model"
      ],
      "metadata": {
        "id": "MzFPsneLn_od"
      },
      "id": "MzFPsneLn_od"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "1CqVpRUOn8Nz"
      },
      "id": "1CqVpRUOn8Nz",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ],
      "metadata": {
        "id": "Sg4_s799oFmz"
      },
      "id": "Sg4_s799oFmz",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpS8CMq4oI1J",
        "outputId": "3debc8e9-876c-4779-8a95-2ce68aab762a"
      },
      "id": "DpS8CMq4oI1J",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 41925787.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 256)  # Adjust input size\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 64 * 14 * 14)  # Flattening the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n"
      ],
      "metadata": {
        "id": "3eF6d9NfphLp"
      },
      "id": "3eF6d9NfphLp",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the neural network model\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.fc1 = nn.Linear(64 * 5 * 5, 256)\n",
        "#         self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       x = self.pool(torch.relu(self.conv1(x)))\n",
        "#       x = x.view(-1, 64 * 14 * 14)  # Adjust the size here\n",
        "#       x = torch.relu(self.fc1(x))\n",
        "#       x = self.fc2(x)\n",
        "#       return x\n",
        "\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.pool(torch.relu(self.conv1(x)))\n",
        "    #     # x = x.view(-1, 64 * 5 * 5)\n",
        "    #     print(x.size())  # Add this line to check the size of x\n",
        "    #     # x = x.view(-1, correct_size)\n",
        "\n",
        "    #     x = torch.relu(self.fc1(x))\n",
        "    #     x = self.fc2(x)\n",
        "    #     return x\n",
        "\n",
        "# net = Net()"
      ],
      "metadata": {
        "id": "kgC9E1GRoIs4"
      },
      "id": "kgC9E1GRoIs4",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(10):  # Adjust the number of epochs as needed\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQi5zi0poPrO",
        "outputId": "507159fe-39c5-4b4c-ec26-1679cbff101b"
      },
      "id": "wQi5zi0poPrO",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.9152058277593549\n",
            "Epoch 2, Loss: 1.622684334702504\n",
            "Epoch 3, Loss: 1.4952651908635484\n",
            "Epoch 4, Loss: 1.4250439022050794\n",
            "Epoch 5, Loss: 1.3720774700879441\n",
            "Epoch 6, Loss: 1.3248352408409119\n",
            "Epoch 7, Loss: 1.2803676947760765\n",
            "Epoch 8, Loss: 1.2369671256645867\n",
            "Epoch 9, Loss: 1.200069046919913\n",
            "Epoch 10, Loss: 1.1693911524989722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "net.eval()\n",
        "all_predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = accuracy_score(true_labels, all_predictions)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0JknYqAn8QZ",
        "outputId": "3450bed9-d7ca-40f4-a5ef-73d2021ac022"
      },
      "id": "X0JknYqAn8QZ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 62.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the model to a file named 'original_model.pth'\n",
        "torch.save(net.state_dict(), 'original_model.pth')"
      ],
      "metadata": {
        "id": "DXwSNm8g1ja_"
      },
      "id": "DXwSNm8g1ja_",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model as a TorchScript module\n",
        "scripted_model = torch.jit.script(net)\n",
        "scripted_model.save(\"optimized_model.pt\")\n",
        "\n",
        "# Load the saved TorchScript model for inference\n",
        "loaded_model = torch.jit.load(\"optimized_model.pt\")"
      ],
      "metadata": {
        "id": "pUf9wKU5ymmz"
      },
      "id": "pUf9wKU5ymmz",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the optimized model on the test set\n",
        "loaded_model.eval()\n",
        "all_predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = loaded_model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "accuracy = accuracy_score(true_labels, all_predictions)\n",
        "print(f'Test Accuracy (Optimized Model): {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fegaA1goy1ko",
        "outputId": "62c04262-cb17-4e3b-a940-48fb50e95506"
      },
      "id": "fegaA1goy1ko",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (Optimized Model): 62.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# net\n",
        "# scripted_model\n",
        "\n",
        "# Define the number of inference runs\n",
        "num_runs = 100\n",
        "\n",
        "# Measure inference time for the PyTorch model\n",
        "pytorch_inference_times = []\n",
        "for _ in range(num_runs):\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        outputs = net(images)  # Replace 'model' with your PyTorch model\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "        pytorch_inference_times.append(inference_time)\n",
        "\n",
        "# Measure inference time for the TorchScript-optimized model\n",
        "torchscript_inference_times = []\n",
        "for _ in range(num_runs):\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        outputs = scripted_model(images)  # Replace 'scripted_model' with your TorchScript model\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "        torchscript_inference_times.append(inference_time)\n",
        "\n",
        "# Calculate and print average inference times\n",
        "avg_pytorch_inference_time = sum(pytorch_inference_times) / num_runs\n",
        "avg_torchscript_inference_time = sum(torchscript_inference_times) / num_runs\n",
        "\n",
        "print(f\"Average PyTorch Inference Time: {avg_pytorch_inference_time:.5f} seconds\")\n",
        "print(f\"Average TorchScript Inference Time: {avg_torchscript_inference_time:.5f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr8pWaChzuQ4",
        "outputId": "3ca6fc4d-5565-40be-905a-561c81ae40d9"
      },
      "id": "Fr8pWaChzuQ4",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PyTorch Inference Time: 0.01923 seconds\n",
            "Average TorchScript Inference Time: 0.01789 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Define data transformations for the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the image\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset (test set)\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Choose an index to select an image from the dataset\n",
        "image_index = 0  # Change this to the index you want\n",
        "\n",
        "# Get the selected image and its label\n",
        "input_image, label = testset[image_index]\n",
        "\n",
        "# Print the label (class) of the selected image\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoN7-Kl-0rsg",
        "outputId": "ef0f85c7-7f52-46fc-c1d8-a0971f98f610"
      },
      "id": "AoN7-Kl-0rsg",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Measure latency for the PyTorch model\n",
        "with torch.no_grad():\n",
        "    start_time = time.time()\n",
        "    output = net(input_image)  # Replace 'model' with your PyTorch model\n",
        "    end_time = time.time()\n",
        "    latency_pytorch = end_time - start_time\n",
        "\n",
        "# Measure latency for the TorchScript-optimized model\n",
        "with torch.no_grad():\n",
        "    start_time = time.time()\n",
        "    output = scripted_model(input_image)  # Replace 'scripted_model' with your TorchScript model\n",
        "    end_time = time.time()\n",
        "    latency_torchscript = end_time - start_time\n",
        "\n",
        "# Compare model file sizes\n",
        "original_model_size = os.path.getsize('original_model.pth')  # Replace with the actual file path\n",
        "torchscript_model_size = os.path.getsize('optimized_model.pt')  # Replace with the actual file path\n",
        "\n",
        "print(f\"Latency (PyTorch): {latency_pytorch:.5f} seconds\")\n",
        "print(f\"Latency (TorchScript): {latency_torchscript:.5f} seconds\")\n",
        "print(f\"Original Model Size: {original_model_size / (1024 * 1024):.2f} MB\")\n",
        "print(f\"TorchScript Model Size: {torchscript_model_size / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2vzpp9i0AwC",
        "outputId": "23f3ad0a-533c-4969-c116-ae115cf9931b"
      },
      "id": "A2vzpp9i0AwC",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency (PyTorch): 0.01892 seconds\n",
            "Latency (TorchScript): 0.00291 seconds\n",
            "Original Model Size: 12.28 MB\n",
            "TorchScript Model Size: 12.29 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BOOZag7k1PP-"
      },
      "id": "BOOZag7k1PP-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}